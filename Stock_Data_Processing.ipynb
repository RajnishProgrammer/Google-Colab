{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxuB/YvZRJHQ0aCxJhVoTY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajnishProgrammer/Google-Colab/blob/main/Stock_Data_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Large-Scale Data Processing Pipeline\n",
        "## Tech Stack: Python, Apache Kafka, Apache Spark, PostgreSQL, AWS S3\n",
        "---\n",
        "Description: Build a system that ingests, processes, and stores large volumes of data in real time.\n",
        "\n",
        "Use Case: Useful for businesses handling streaming data (e.g., stock market, IoT data, social media feeds).\n",
        "\n",
        "Key Features:\n",
        "- Fetch data from APIs or Kafka streams.\n",
        "- Process and clean data using Pandas/Spark.\n",
        "- Store processed data in a database (PostgreSQL/BigQuery).\n",
        "- Expose REST APIs for accessing processed data.\n",
        "\n"
      ],
      "metadata": {
        "id": "aEpJ8Cl670HT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“Œ Step 1: Define the Use Case\n",
        "\n",
        "Before implementation, decide what kind of data you want to process.\n",
        "\n",
        "Some options:\n",
        "\n",
        "- Real-time stock market data ðŸ¦ (from Alpha Vantage API)\n",
        "- Twitter sentiment analysis ðŸ“¢ (via Twitter API)\n",
        "- IoT sensor data processing ðŸ­ (from MQTT brokers)\n",
        "- Log processing for system monitoring ðŸ“Š (server logs)"
      ],
      "metadata": {
        "id": "Yi9fIFfH9mBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ›  Step 2: Tech Stack\n",
        "\n",
        "- Python â†’ Core language for data processing\n",
        "- Apache Kafka â†’ Message queue for real-time streaming\n",
        "- Apache Spark â†’ Distributed data processing\n",
        "- PostgreSQL / BigQuery â†’ Database for storage\n",
        "- FastAPI / Flask â†’ API layer to expose processed data\n",
        "- AWS S3 â†’ Storage for raw data"
      ],
      "metadata": {
        "id": "7eIXAHvd93Xg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸš€ Step 3: Build the Pipeline\n",
        "\n",
        "1ï¸âƒ£ Data Ingestion\n",
        "\n",
        "2ï¸âƒ£ Data Processing & Transformation\n",
        "\n",
        "3ï¸âƒ£ Store Processed Data\n",
        "\n",
        "4ï¸âƒ£ Expose the Data via API\n"
      ],
      "metadata": {
        "id": "n3-tROny-LmK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGBOmeQj45R3"
      },
      "outputs": [],
      "source": [
        "# 01 Data Ingestion\n",
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "API_URL = 'https://www.alphavantage.co/query'\n",
        "API_KEY = 'AHDD20EVTIKJ1QJB'\n",
        "\n",
        "def fetch_stock_data(symbol):\n",
        "  params = {\n",
        "    'function': 'TIME_SERIES_INTRADAY',\n",
        "    'symbol': symbol,\n",
        "    'interval': '1min',\n",
        "    'apikey': API_KEY\n",
        "  }\n",
        "  response = requests.get(API_URL, params=params)\n",
        "  return response.json()\n",
        "\n",
        "json_data = fetch_stock_data('IBM')\n",
        "json_str = json.dumps(json_data)\n",
        "data = pd.read_json(json_str)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['Meta Data']"
      ],
      "metadata": {
        "id": "_bJrj5E151ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Time Series (1min)']"
      ],
      "metadata": {
        "id": "BNTBSYHy51en"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 02 Data Processing & Transformation\n",
        "def clean_data(raw_data):\n",
        "  df = pd.DataFrame(raw_data['Time Series (1min)']).T\n",
        "  df = df.astype(float)\n",
        "  df.index = pd.to_datetime(df.index)\n",
        "  return df\n",
        "\n",
        "\n",
        "cleaned_data = clean_data(fetch_stock_data('IBM'))\n",
        "print(cleaned_data)\n",
        "print('Done Cleaning...')"
      ],
      "metadata": {
        "id": "7Yu7_Uft51ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_data.columns = ['open', 'high', 'low', 'close', 'volume']"
      ],
      "metadata": {
        "id": "o4JiKsuc51U1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_data = cleaned_data.reset_index()"
      ],
      "metadata": {
        "id": "v0nmxFDS51R_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_data = cleaned_data.rename(columns={'index': 'timestamp'})"
      ],
      "metadata": {
        "id": "UX000q7X51O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw = fetch_stock_data('IBM')\n",
        "raw['Time Series (1min)']"
      ],
      "metadata": {
        "id": "G1cLkjTK51Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = pd.DataFrame(raw['Time Series (1min)']).T\n",
        "temp"
      ],
      "metadata": {
        "id": "ympOJe3X51I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = temp.astype(float)\n",
        "temp"
      ],
      "metadata": {
        "id": "PeAfbJJp51Fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp.index = pd.to_datetime(temp.index)\n",
        "temp"
      ],
      "metadata": {
        "id": "xLZrWXgy50_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 03 Store processed data\n",
        "import psycopg2\n",
        "\n",
        "conn = psycopg2.connect(\"postgresql://postgres:wSiOhjRgzIPRzJYzFLeGUCQSiLGRtvQs@junction.proxy.rlwy.net:53289/railway\")\n",
        "\n",
        "cursor = conn.cursor()\n",
        "cursor.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS stock_prices(\n",
        "      timestamp TIMESTAMP PRIMARY KEY,\n",
        "      open_price FLOAT,\n",
        "      close_price FLOAT\n",
        "    )\n",
        "\"\"\")\n",
        "conn.commit()"
      ],
      "metadata": {
        "id": "CPXbr2x7502S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in cleaned_data.iterrows():\n",
        "\n",
        "  cursor.execute(\"\"\"\n",
        "      INSERT INTO stock_prices (timestamp, open_price, close_price)\n",
        "      VALUES (%s, %s, %s)\n",
        "  \"\"\", (row['timestamp'], row['open'], row['close']))\n",
        "\n",
        "conn.commit()"
      ],
      "metadata": {
        "id": "gn0pqrZN50rJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install fastapi"
      ],
      "metadata": {
        "id": "P_3Q69DR6Q-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pyngrok\n",
        "! pip install uvicorn"
      ],
      "metadata": {
        "id": "pS3jXP4j6Q7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 04 Expose the data via API\n",
        "from fastapi import FastAPI\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "import psycopg2\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "def get_latest_data():\n",
        "  conn = psycopg2.connect(\"postgresql://postgres:wSiOhjRgzIPRzJYzFLeGUCQSiLGRtvQs@junction.proxy.rlwy.net:53289/railway\")\n",
        "  cursor = conn.cursor()\n",
        "  cursor.execute(\"SELECT * FROM stock_prices ORDER BY timestamp DESC LIMIT 10\")\n",
        "  return cursor.fetchall()\n",
        "\n",
        "@app.get('/')\n",
        "def index():\n",
        "  return {'result': 'Try this endpoint ---> /latest-prices'}\n",
        "\n",
        "@app.get(\"/latest-prices\")\n",
        "def latest_prices():\n",
        "  return get_latest_data()"
      ],
      "metadata": {
        "id": "6wmONaJJ6Q4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "Qo5iKNLJ6Q15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ngrok config add-authtoken 2sXnC1k3Jv39N7S4dk58PF7AKLR_7kugrF1FyFLkp3Pg9np7a"
      ],
      "metadata": {
        "id": "dRouIqIA6Qy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "public_url = ngrok.connect(addr=\"8000\", proto=\"http\")\n",
        "print(f'Public URL: {public_url}')"
      ],
      "metadata": {
        "id": "6Ax52I8U6Qwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uvicorn.run(app, host='0.0.0.0', port=8000)"
      ],
      "metadata": {
        "id": "H7RVyt5a6QJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "# !pip install fastapi uvicorn nest-asyncio pyngrok psycopg2\n",
        "\n",
        "# Import necessary modules\n",
        "from fastapi import FastAPI\n",
        "import psycopg2\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Connect to PostgreSQL database\n",
        "conn = psycopg2.connect(\"postgresql://postgres:wSiOhjRgzIPRzJYzFLeGUCQSiLGRtvQs@junction.proxy.rlwy.net:53289/railway\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Define endpoint to get latest stock prices\n",
        "@app.get('/')\n",
        "def index():\n",
        "  return 'Hello World !!'\n",
        "\n",
        "@app.get(\"/latest-prices\")\n",
        "def latest_prices():\n",
        "    cursor.execute(\"SELECT * FROM stock_prices ORDER BY timestamp DESC LIMIT 10\")\n",
        "    result = cursor.fetchall()\n",
        "    return {\"data\": result}\n",
        "\n",
        "# Apply nest_asyncio to avoid event loop issues in Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Authenticate ngrok (Replace with your actual authtoken)\n",
        "!ngrok authtoken 2sXnC1k3Jv39N7S4dk58PF7AKLR_7kugrF1FyFLkp3Pg9np7a\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(addr=\"8000\", proto=\"http\")\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "# Run FastAPI app\n",
        "uvicorn.run(app, host=\"localhost\", port=8000)"
      ],
      "metadata": {
        "id": "dpeGUrJf6wj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "â³ Step 4: Automate & Deploy\n",
        "\n",
        "ðŸ”¹ Automate with Airflow\n",
        "\n",
        "ðŸ”¹ Deploy to Cloud (AWS, GCP, or Azure)"
      ],
      "metadata": {
        "id": "eMsCCc2w-p1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Automate & Deploy\n",
        "from airflow import DAG\n",
        "from airflow.operators.python_operator import PythonOperator\n",
        "from datetime import datetime\n",
        "\n",
        "dag = DAG(\"data_pipeline\", start_date=datetime(2025, 2, 4), schedule_interval=\"@hourly\")\n",
        "\n",
        "fetch_task = PythonOperator(\n",
        "    task_id=\"fetch_stock_data\",\n",
        "    python_callable=fetch_stock_data,\n",
        "    op_args=['IBM'],\n",
        "    dag=dag\n",
        ")"
      ],
      "metadata": {
        "id": "h6La-IUM6wZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸš€ Deploy to cloud >\n",
        "- use docker\n",
        "- deploy api's on aws lambda or EC2 instance\n",
        "- use aws rds/big query for databases"
      ],
      "metadata": {
        "id": "a56sMIIy65S8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ”® What else:\n",
        "- Add ml predict trends\n",
        "- process more data sources\n",
        "- Use kafka stream for better performance"
      ],
      "metadata": {
        "id": "dn8sRx9-69he"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# THE END"
      ],
      "metadata": {
        "id": "lYEGidQZ6u2E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}